{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Federated_Learning .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phanirohith/Federated-Learning-through-Distance-Based-Clustering/blob/main/Federated_Learning_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOy-z9DnV3Ow"
      },
      "source": [
        "!pip install tensorflow-federated\n",
        "\n",
        "import tensorflow_federated as tff\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "import collections\n",
        "import attr\n",
        "import functools\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.cluster import KMeans\n",
        "from matplotlib import pyplot as plt\n",
        "import nest_asyncio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihCSOkUnWCHP"
      },
      "source": [
        "np.random.seed(0)\n",
        "nest_asyncio.apply()\n",
        "tf.compat.v1.enable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPOCGGtWWHEj"
      },
      "source": [
        "# Constants\n",
        "NUMBER_OF_CLIENTS = 100\n",
        "BATCH_SIZE = 10\n",
        "NUMBER_OF_EPOCHS = 20\n",
        "NUMBER_OF_CLUSTERS = 5\n",
        "# use only for dynamic clustering\n",
        "SHUFFLE_BUFFER = 100\n",
        "PREFETCH_BUFFER = 10\n",
        "USE_KMEANS = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF0W3ISWWKlU"
      },
      "source": [
        "emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()\n",
        "\n",
        "def preprocess(dataset):\n",
        "  def batch_format_fn(element):\n",
        "    return collections.OrderedDict(\n",
        "        x=tf.reshape(element['pixels'],[-1,28,28,1]),\n",
        "        y=tf.reshape(element['label'], [-1, 1]))\n",
        "\n",
        "  return dataset.repeat(NUMBER_OF_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(\n",
        "      BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)\n",
        "\n",
        "def preprocess_val(dataset):\n",
        "  def batch_format_fn(element):\n",
        "    return (tf.reshape(element['pixels'],(-1,28,28,1)),\n",
        "            tf.reshape(element['label'],[-1,1]))\n",
        "  return dataset.batch(BATCH_SIZE).map(batch_format_fn)\n",
        "  \n",
        "def get_dataset_for_client(client_id, dataset):\n",
        "  return preprocess(dataset.create_tf_dataset_for_client(client_id))\n",
        "\n",
        "# Get device ids\n",
        "device_ids = emnist_train.client_ids\n",
        "np.random.shuffle(device_ids)\n",
        "device_ids = device_ids[:NUMBER_OF_CLIENTS]\n",
        "\n",
        "# Get the device data\n",
        "train_device_datasets = [get_dataset_for_client(device_id, emnist_train) for device_id in device_ids]\n",
        "test_device_datasets = [get_dataset_for_client(device_id, emnist_test) for device_id in device_ids]\n",
        "central_emnist_test = emnist_test.create_tf_dataset_from_all_clients().take(1000)\n",
        "central_emnist_test = preprocess_val(central_emnist_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM0GxFDpWQSF"
      },
      "source": [
        "# Define the model\n",
        "def keras_model():\n",
        "  return tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28,28,1)),                                 \n",
        "    tf.keras.layers.Conv2D(32,kernel_size=(5,5),activation='relu'),\n",
        "    tf.keras.layers.Conv2D(64,kernel_size=(5,5),activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2,2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Softmax(),\n",
        "  ])\n",
        "\n",
        "\n",
        "# Define TFF wrapper\n",
        "def wrap_model_with_tff(model, input_spec):\n",
        "  return tff.learning.from_keras_model(\n",
        "    model, input_spec = input_spec,\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "input_spec = train_device_datasets[0].element_spec\n",
        "\n",
        "# FL Training\n",
        "@tf.function\n",
        "def server_update(model, mean_clients_weights):\n",
        "  model_weights = model.weights.trainable\n",
        "\n",
        "  tf.nest.map_structure(lambda x,y: x.assign(y),\n",
        "                        model_weights,mean_clients_weights)\n",
        "  return model_weights\n",
        "\n",
        "@tff.tf_computation\n",
        "def server_init():\n",
        "  tff_model = wrap_model_with_tff(keras_model(), input_spec)\n",
        "  return tff_model.weights.trainable\n",
        "\n",
        "model_weights_type = server_init.type_signature.result\n",
        "tf_dataset_type = tff.SequenceType(input_spec)\n",
        "\n",
        "@tff.tf_computation(model_weights_type)\n",
        "def server_update_fn(mean_client_weights):\n",
        "  tff_model = wrap_model_with_tff(keras_model(), input_spec)\n",
        "  return server_update(tff_model, mean_client_weights)\n",
        "\n",
        "@tf.function\n",
        "def client_update(model, dataset, server_weights, client_optimizer):\n",
        "  #Initialize the client weights with the server weights\n",
        "  client_weights = model.weights.trainable\n",
        "\n",
        "  #Assign the server weights to the client model\n",
        "  tf.nest.map_structure(lambda x,y: x.assign(y),\n",
        "                        client_weights,server_weights)\n",
        "  #Use the client optimizer to update the local model.\n",
        "  for batch in dataset:\n",
        "    with tf.GradientTape() as tape:\n",
        "      #compute a forward pass on the batch of data\n",
        "      outputs = model.forward_pass(batch)\n",
        "    #compute the corresponding gradient\n",
        "    grads = tape.gradient(outputs.loss, client_weights)\n",
        "    grads_and_vars = zip(grads, client_weights)\n",
        "    \n",
        "    #apply the gradient using client optimizer\n",
        "    client_optimizer.apply_gradients(grads_and_vars)\n",
        "\n",
        "  return client_weights\n",
        "\n",
        "@tff.tf_computation(tf_dataset_type, model_weights_type)\n",
        "def client_update_fn(tf_dataset, server_weights):\n",
        "  tff_model = wrap_model_with_tff(keras_model(), input_spec)\n",
        "  # client_optimizer = tf.keras.optimizers.Adam()\n",
        "  client_optimizer = tf.keras.optimizers.SGD(lr=0.1)\n",
        "  return client_update(tff_model, tf_dataset, server_weights, client_optimizer)\n",
        "\n",
        "federated_server_type = tff.FederatedType(model_weights_type, tff.SERVER)\n",
        "federated_dataset_data = tff.FederatedType(tf_dataset_type, tff.CLIENTS)\n",
        "\n",
        "@tff.federated_computation(federated_server_type, federated_dataset_data)\n",
        "def next_fn(server_weights, federated_dataset):\n",
        "  # Send server weights to clients\n",
        "  server_weights_to_clients = tff.federated_broadcast(server_weights)\n",
        "\n",
        "  # Each client computes their updated weights\n",
        "  client_weights = tff.federated_map(client_update_fn, (federated_dataset,server_weights_to_clients))\n",
        "\n",
        "  # Client mean\n",
        "  mean_client_weights = tff.federated_mean(client_weights)\n",
        "\n",
        "  # Server averages all the client weights\n",
        "  mean_client_weights = tff.federated_mean(client_weights)\n",
        "\n",
        "  # The server updates it model \n",
        "  server_weights = tff.federated_map(server_update_fn, mean_client_weights)\n",
        "\n",
        "  return (server_weights, client_weights)\n",
        "\n",
        "@tff.tf_computation(model_weights_type)\n",
        "def client_work(model_weights):\n",
        "  return model_weights\n",
        "\n",
        "@tff.federated_computation(tff.FederatedType(model_weights_type, tff.CLIENTS))\n",
        "def run_one_round(weights):\n",
        "  tff_model = wrap_model_with_tff(keras_model(), input_spec)\n",
        "  return tff.federated_map(client_work, weights)\n",
        "\n",
        "@tff.federated_computation\n",
        "def initialize_fn():\n",
        "  return tff.federated_value(server_init(), tff.SERVER)\n",
        "\n",
        "federated_algorithm = tff.templates.IterativeProcess(\n",
        "    initialize_fn = initialize_fn, next_fn = next_fn\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYxe0L7DWUjd"
      },
      "source": [
        "# Phase 1\n",
        "def evaluate(server_state, dataset = central_emnist_test):\n",
        "  model = keras_model()\n",
        "  model.compile(\n",
        "      loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "  )\n",
        "  model.set_weights(server_state)\n",
        "  return model.evaluate(dataset)\n",
        "\n",
        "# Train as a traditional FL model\n",
        "server_state = federated_algorithm.initialize()\n",
        "updated_server_state_phase_1 = server_state\n",
        "updated_client_weights = []\n",
        "for i in tqdm(range(0, NUMBER_OF_EPOCHS)):\n",
        "  result = federated_algorithm.next(updated_server_state_phase_1, train_device_datasets)\n",
        "  updated_server_state_phase_1 = result[0]\n",
        "  updated_client_weights = result[1]\n",
        "  evaluate(updated_server_state_phase_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laATyO9vWZ7R"
      },
      "source": [
        "# Phase 2\n",
        "# Cluster the weights from Phase 1\n",
        "client_weights_flat = [client_weight[0].numpy().reshape(-1) for client_weight in updated_client_weights]\n",
        "number_of_clusters = NUMBER_OF_CLUSTERS\n",
        "kmeans = KMeans(n_clusters=number_of_clusters)\n",
        "kmeans.fit(client_weights_flat)\n",
        "clusterd_weights_indexes = kmeans.predict(client_weights_flat)\n",
        "\n",
        "if not USE_KMEANS:\n",
        "  # Dynamic clustering\n",
        "  THRESHOLD = 0.15\n",
        "  client_distances = np.ndarray((NUMBER_OF_CLIENTS, NUMBER_OF_CLIENTS))\n",
        "  for i in range(0, NUMBER_OF_CLIENTS):\n",
        "    for j in range(0, NUMBER_OF_CLIENTS):\n",
        "      client_distances[i, j] = np.linalg.norm(client_weights_flat[i] - client_weights_flat[j])\n",
        "  # THRESHOLD = np.average(client_distances)\n",
        "\n",
        "  current_cluster_index = 0\n",
        "  device_cluster_map = []\n",
        "  clustered_devices = set()\n",
        "  for i in range(0, NUMBER_OF_CLIENTS):\n",
        "    # if i in clustered_devices:\n",
        "    #   continue\n",
        "    if len(device_cluster_map) <= current_cluster_index:\n",
        "      device_cluster_map.append([i])\n",
        "    clustered_devices.add(i)\n",
        "    for j in range(0, NUMBER_OF_CLIENTS):\n",
        "      if(i == j):\n",
        "        continue\n",
        "      else:\n",
        "        if(client_distances[i][j] < THRESHOLD):\n",
        "          device_cluster_map[current_cluster_index].append(j)\n",
        "          clustered_devices.add(j)\n",
        "    current_cluster_index = current_cluster_index + 1\n",
        "  NUMBER_OF_CLUSTERS = current_cluster_index - 1\n",
        "\n",
        "  # Dynamic clustering\n",
        "  cluster_device_map = {}\n",
        "  cluster_device_weight_map = {}\n",
        "  cluster_device_datasets = {}\n",
        "  for cluster_index in range(0, len(device_cluster_map)):\n",
        "    cluster_key = str(cluster_index)\n",
        "    cluster_device_map[cluster_key] = device_cluster_map[cluster_index]\n",
        "    if cluster_key not in cluster_device_weight_map:\n",
        "      cluster_device_weight_map[cluster_key] = []\n",
        "      cluster_device_datasets[cluster_key] = []\n",
        "    for device_index in cluster_device_map[cluster_key]:\n",
        "      cluster_device_weight_map[cluster_key].append(updated_client_weights[device_index])\n",
        "      cluster_device_datasets[cluster_key].append(train_device_datasets[device_index])\n",
        "else:\n",
        "  # Calculate the each of the cluster's weights\n",
        "  cluster_device_map = {}\n",
        "  cluster_device_weight_map = {}\n",
        "  cluster_device_datasets = {}\n",
        "  for i in range(0, len(clusterd_weights_indexes)):\n",
        "    cluster_key = str(clusterd_weights_indexes[i])\n",
        "    if cluster_key not in cluster_device_map:\n",
        "      cluster_device_map[cluster_key] = []\n",
        "      cluster_device_weight_map[cluster_key] = []\n",
        "      cluster_device_datasets[cluster_key] = []\n",
        "    cluster_device_map[cluster_key].append(i)\n",
        "    cluster_device_weight_map[cluster_key].append(updated_client_weights[i])\n",
        "    cluster_device_datasets[cluster_key].append(train_device_datasets[i])\n",
        "\n",
        "# Calculate the averages of the clusters\n",
        "cluster_device_average_weights = {}\n",
        "for cluster_key in cluster_device_weight_map.keys():\n",
        "  cluster_weights = np.array(cluster_device_weight_map[cluster_key])\n",
        "  cluster_size = len(cluster_weights)\n",
        "  cluster_sums = []\n",
        "  for i in range(0, len(cluster_weights[0])):\n",
        "    cluster_sums.append(np.add.reduce(np.array(cluster_weights)[:, i])/cluster_size)\n",
        "  cluster_device_average_weights[cluster_key] = cluster_sums\n",
        "\n",
        "# Clustered Training\n",
        "# ToDo: How to set weights of indiviudal clients?\n",
        "cluster_weights_after_training = {}\n",
        "cluster_accuracies = {}\n",
        "for cluster_key in cluster_device_average_weights.keys():\n",
        "  server_state = federated_algorithm.initialize()\n",
        "  server_update_fn(cluster_device_average_weights[cluster_key])\n",
        "  updated_server_state = cluster_device_average_weights[cluster_key]\n",
        "  run_one_round(updated_client_weights)\n",
        "  updated_client_weights_curr = []\n",
        "  cluster_eval_results = []\n",
        "  for i in tqdm(range(0, NUMBER_OF_EPOCHS)):\n",
        "    result = federated_algorithm.next(updated_server_state, cluster_device_datasets[cluster_key])\n",
        "    updated_server_state = result[0]\n",
        "    updated_client_weights_curr = result[1]\n",
        "    cluster_eval_results = evaluate(updated_server_state)\n",
        "  # cluster_weights_after_training[cluster_key] = (updated_server_state, updated_client_weights_curr)\n",
        "  cluster_weights_after_training[cluster_key] = updated_server_state\n",
        "  cluster_accuracies[cluster_key] = cluster_eval_results[1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzagJukDoLYT"
      },
      "source": [
        "cluster_weights_sums = None\n",
        "cluster_weights_sums_weighted = None\n",
        "max_accuracy = np.max(np.array(list(cluster_accuracies.values())))\n",
        "for cluster_key in cluster_weights_after_training.keys():\n",
        "  curr_cluster_weights = np.array(cluster_weights_after_training[cluster_key])\n",
        "  curr_accuracy_factor = cluster_accuracies[cluster_key] / max_accuracy\n",
        "  if cluster_weights_sums is None:\n",
        "    cluster_weights_sums = curr_cluster_weights\n",
        "    cluster_weights_sums_weighted = curr_cluster_weights * curr_accuracy_factor\n",
        "  else:\n",
        "    for i in range(len(cluster_weights_sums)):\n",
        "      cluster_weights_sums[i] = cluster_weights_sums[i] + curr_cluster_weights[i]\n",
        "      cluster_weights_sums_weighted[i] = cluster_weights_sums_weighted[i] + curr_cluster_weights[i] * curr_accuracy_factor\n",
        "cluster_weights_average = cluster_weights_sums / len(list(cluster_device_map.keys()))\n",
        "cluster_weights_weighted_average = cluster_weights_sums_weighted / len(list(cluster_device_map.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBbmkbQJWo0b"
      },
      "source": [
        "results = {\n",
        "  \"Phase 1 eval\": evaluate(updated_server_state_phase_1),\n",
        "  \"Phase 3 avg eval\": evaluate(cluster_weights_average),\n",
        "  \"Phase 3 acc combination avg eval\": evaluate(cluster_weights_weighted_average),\n",
        "}\n",
        "\n",
        "# Phase 1 device results\n",
        "device_id_ref = 0\n",
        "for device_weight in updated_client_weights:\n",
        "  # results[f\"Phase1 - Device {device_id_ref}:\"] = evaluate(device_weight)\n",
        "  results[f\"Phase1 - Device {device_id_ref}:\"] = evaluate(\n",
        "      device_weight,\n",
        "      dataset = preprocess_val(emnist_test.create_tf_dataset_for_client(device_ids[device_id_ref]))\n",
        "      )\n",
        "  device_id_ref+=1\n",
        "\n",
        "# Results after clustering\n",
        "for cluster_key in cluster_device_map:\n",
        "  for device_id in cluster_device_map[cluster_key]:\n",
        "    results[f\"Cluster {cluster_key} with device {device_id}:\"] = evaluate((\n",
        "        cluster_weights_after_training[cluster_key] + cluster_weights_weighted_average) / 2, \n",
        "        dataset = preprocess_val(emnist_test.create_tf_dataset_for_client(device_ids[device_id])\n",
        "      ))\n",
        "\n",
        "for result in results:\n",
        "  print(f\"{result}: {results[result]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28p236iEp6nk"
      },
      "source": [
        "# Phase 1 device accuracies\n",
        "phase1_device_acc = []\n",
        "for client in range(NUMBER_OF_CLIENTS):\n",
        "  phase1_device_acc.append((results[f'Phase1 - Device {client}:'])[1])\n",
        "\n",
        "# Phase 3 device accuracies\n",
        "phase3_device_acc = []\n",
        "for client in range(NUMBER_OF_CLIENTS):\n",
        "  search_key = f'with device {client}'\n",
        "  phase3_device_acc.append([val for key, val in results.items() if search_key in key][0][1])\n",
        "\n",
        "print(np.count_nonzero(np.array(phase1_device_acc) > 0.95))\n",
        "print(np.count_nonzero(np.array(phase3_device_acc) > 0.95))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V0eo5cSnaWh"
      },
      "source": [
        "plt.figure(figsize=(14,7))\n",
        "\n",
        "data = [phase1_device_acc],[phase3_device_acc]\n",
        "p1 = plt.bar(ind+0.00, phase1_device_acc, 0.4)\n",
        "p2 = plt.bar(ind+0.4, phase3_device_acc, 0.4)\n",
        "\n",
        "plt.ylabel('Device Accuracy')\n",
        "plt.xlabel('Devices')\n",
        "plt.title('Device accuracies in phase 1 and 3')\n",
        "plt.xticks(ind)\n",
        "plt.legend((p1[0], p2[0]), ('Phase1', 'Phase3'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}